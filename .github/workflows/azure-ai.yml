# This is a basic workflow to help you get started with Actions

name: MLOps Azure Data Preparing and AI Training

# Controls when the workflow will run
on:
  # Triggers the workflow on push or pull request events but only for the main branch
  push:
    branches: [ main, lessonsteps/** ]
  pull_request:
    branches: [ main, lessonsteps/** ]
    

env:
  # TENANT_ID: ${{ secrets.TENANT_ID }}
  # CLIENT_ID: ${{ secrets.CLIENT_ID }}
  # CLIENT_SECRET: ${{ secrets.CLIENT_SECRET }}
  # data_prep: false
  # ai_training: true
  # api_creation: false

  WORKSPACE_NAME: segersnathan # Change this !
  RESOURCE_GROUP: NathanReserve # Change this !
  SUBSCRIPTION_ID: 7c50f9c3-289b-4ae0-a075-08784b3b9042 # Change this !

  TEMP_STATE_DIRECTORY: tmp


# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:

  # This job is just to set redirect some environment values as output values.
  dependencies:
    runs-on: ubuntu-20.04

    outputs:
      data_prep: 'false'
      ai_training: 'false'
      api_creation: 'true'

    steps:
      - run: echo 'all outputs have been set'

  data-preparing:
    needs: [dependencies]
    if: ${{ needs.dependencies.outputs.data_prep == 'true' }}
    # The type of runner that the job will run on
    runs-on: ubuntu-20.04

    env:
      DATA_FOLDER: data
      DATASET_NEW_VERSION: true
      DATASET_DESCRIPTION: Animals and their images
      DATASET_BASE_NAME: animals
      ANIMALS: cats, dogs, pandas

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v2

      # - name: 'Set up python'
      #   uses: actions/setup-python@v2
      #   with:
      #     python-version: 3.8
      #     cache: 'pip'
      
      # - name: 'install requirements'
      #   run: pip install -r requirements.txt
          
      # - name: 'Run data prep script'
      #   id: dataprep
      #   working-directory: AI-Pipeline
      #   run: |
      #     python 01_DataPreparing.py
      
      # - name: 'Save config data'
      #   uses: actions/upload-artifact@v2.2.3
      #   with:
      #     name: config-data
      #     path: {{ env.TEMP_STATE_DIRECTORY }}
      #     retention-days: 10

  ai-training:
    needs: [dependencies, data-preparing]
    # Allow this job to start if the previous one succeeded. Or if the ai_training part is set to true specifically.
    if: ${{
        always() &&
        (needs.data-preparing.result == 'success' || needs.data-preparing.result == 'skipped') &&
        needs.dependencies.outputs.ai_training == 'true'
      }}
    runs-on: ubuntu-latest

    env:
      BATCH_SIZE: 64
      EPOCHS: 100
      VALIDATION_SPLIT: 0.2
      TRAIN_SCRIPT_NAME: train.py
      ROOT_DIR: root

    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v2

      # - name: 'Set up python'
      #   uses: actions/setup-python@v2
      #   with:
      #     python-version: 3.8
      #     cache: 'pip'

      # - name: "install requirements"
      #   run: pip install -r requirements.txt


  api-creation-and-CI:
    needs: [dependencies, ai-training]
    # Allow this job to start if the previous one succeeded. Or if the api_creation part is set to true specifically.
    if: ${{
        always() &&
        (needs.ai-training.result == 'success' || needs.ai-training.result == 'skipped') &&
        needs.dependencies.outputs.api_creation == 'true'
      }}
    runs-on: ubuntu-latest

    env:
      MODEL_DIRECTORY: api/models
      MODEL_NAME: animals-cnn
      MODEL_VERSION: latest # change to a specific version if you need to

      DOCKER_TAG: latest
      DOCKER_REPOSITORY: ghcr.io/nathansegers/05-azuremlops

    steps:
      - uses: actions/checkout@v2

      - name: 'Download AI model'
        run: |
          echo "Downloading AI model to model directory"

      - name: 'Docker creation'
        run: |
          echo "Creating Docker image"





