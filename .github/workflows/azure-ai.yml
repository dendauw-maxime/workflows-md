# This is a basic workflow to help you get started with Actions

name: MLOps Azure Data Preparing and AI Training

# Controls when the workflow will run
on:
  # Triggers the workflow on push or pull request events but only for the main branch
  push:
    branches: [ main, lessonsteps/** ]
  pull_request:
    branches: [ main, lessonsteps/** ]

  workflow_dispatch:
    inputs:
      data_prep:
        description: 'Data preparing'
        required: true
        default: "true"
      ai_training:
        description: 'AI Training'
        required: true
        default: "true"
      api_creation:
        description: 'API Creation'
        required: true
        default: "true"
    

env:
  CLIENT_SECRET: ${{ secrets.CLIENT_SECRET }}

  CLIENT_ID: c2f7b98e-f968-48a5-b0d9-f2e8d80edebd # Change this !
  TENANT_ID: 4ded4bb1-6bff-42b3-aed7-6a36a503bf7a # Change this !
  WORKSPACE_NAME: segersnathan # Change this !
  RESOURCE_GROUP: NathanReserve # Change this !
  SUBSCRIPTION_ID: 7c50f9c3-289b-4ae0-a075-08784b3b9042 # Change this !

  ANIMALS: cats,dogs,pandas # As we have three classes, let's just defined them here. This way we can easily loop over them later.
  TRAIN_SET_NAME: animals-training-set
  TEST_SET_NAME: animals-testing-set

  RANDOM_SEED: 42 # Random values that should be same for all the steps

# A workflow run is made up of one or more jobs that can run sequentially or in parallel
jobs:

  # # This job is just to set redirect some environment values as output values.
  # dependencies:
  #   runs-on: ubuntu-20.04

  #   outputs:
  #     data_prep: github.event.inputs.vmSku
  #     ai_training: 'false'
  #     api_creation: 'false'

  #   steps:
  #     - run: echo 'all outputs have been set'

  data-preparing:
    # needs: [dependencies] # Wait until this job was finished.
    if: ${{ github.event.inputs.data_prep == 'true' }}
    # The type of runner that the job will run on
    runs-on: ubuntu-20.04

    env:
      DATA_FOLDER: data
      DATASET_VERSION: true
      TRAIN_TEST_SPLIT_FACTOR: 0.20
      PROCESS_IMAGES: true # Make the pipeline skip processing the images
      SPLIT_IMAGES: true # Make the pipeline skip splitting the images

    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v2

      - name: 'Set up python'
        uses: actions/setup-python@v2
        with:
          python-version: 3.8
          cache: 'pip'
      
      - name: 'install requirements'
        run: pip install -r requirements.txt
          
      - name: 'Run data prep script'
        id: dataprep
        run: |
          python steps/01_DataPreparing.py

  ai-training:
    needs: [data-preparing]
    # This will run Always BUT only when the previous job was successful or skipped && the ai_Training flag is set to true.
    if: ${{
        always() &&
        (needs.data-preparing.result == 'success' || needs.data-preparing.result == 'skipped') &&
        github.event.inputs.ai_training == 'true'
      }}
    runs-on: ubuntu-latest

    env:
      INITIAL_LEARNING_RATE: 0.01
      MAX_EPOCHS: 50
      BATCH_SIZE: 32
      PATIENCE: 11
      MODEL_NAME: animal-cnn
      EXPERIMENT_NAME: animal-classification
      SCRIPT_FOLDER: scripts

      ## Compute cluster parts
      AML_COMPUTE_CLUSTER_NAME: cpu-cluster
      AML_COMPUTE_CLUSTER_MIN_NODES: 0
      AML_COMPUTE_CLISTER_MAX_NODES: 4
      AML_COMPUTE_CLUSTER_SKU: STANDARD_D2_V2

      TRAIN_ON_LOCAL: False # If you want to train on your local runner, set this to True.

      ## Training environment
      CONDA_DEPENDENCIES_PATH: conda_dependencies.yml
      TRAINING_ENV_NAME: animals-classification-env-training

    steps:
      # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it
      - uses: actions/checkout@v2

      - name: 'Set up python'
        uses: actions/setup-python@v2
        with:
          python-version: 3.8
          cache: 'pip'

      - name: "install requirements"
        run: pip install -r requirements.txt

      - name: 'Run data prep script'
        id: aitraining
        run: |
          python steps/02_AITraining.py


  api-creation-and-CI:
    needs: [data-preparing, ai-training]
    # This will run Always BUT only when the previous two jobs were successful or skipped && the api_creation flag is set to true.
    if: ${{
        always() &&
        (needs.data-preparing.result == 'success' || needs.data-preparing.result == 'skipped') &&
        (needs.ai-training.result == 'success' || needs.ai-training.result == 'skipped') &&
        github.event.inputs.api_creation == 'true'
      }}
    runs-on: ubuntu-latest

    env:
      DEPLOYMENT_DEPENDENCIES: deployment_environment.yml
      DEPLOYMENT_ENV_NAME: animals-classification-env-deployment
      SCORE_SERVICE_NAME: animals-classification-svc
      LOCAL_MODEL_PATH: api/outputs
      LOCAL_DEPLOYMENT: 'true'

      DOCKER_TAG: latest
      DOCKER_REPOSITORY: ghcr.io/nathansegers/05-azuremlops-api

    steps:
      - uses: actions/checkout@v2
      
      - name: 'Set up python'
        uses: actions/setup-python@v2
        with:
          python-version: 3.8
          cache: 'pip'

      - name: "install requirements"
        run: pip install -r requirements.txt

      - name: 'Run deployment script'
        id: deployment
        run: |
          python steps/03_Deployment.py


      - name: 'Docker creation'
        if: env.LOCAL_DEPLOYMENT == 'true'
        run: |
          echo "Creating Docker image"





